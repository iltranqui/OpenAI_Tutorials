{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is overfitting and can you also provide an example with the iris dataset\n",
    "\n",
    "Overfitting is a phenomenon that occurs when a machine learning model is trained too well on the training data, and as a result, it performs poorly on unseen data. This happens because the model has learned the noise and random fluctuations in the training data, rather than the underlying patterns and relationships that are present.\n",
    "\n",
    "For example, consider the iris dataset, which consists of 150 samples of iris flowers, with 50 samples from each of three species: setosa, versicolor, and virginica. Each sample has four features: sepal length, sepal width, petal length, and petal width. The goal is to build a model to classify the species of iris based on the features.\n",
    "\n",
    "If we train a machine learning model on the iris dataset and achieve very high accuracy on the training data, but the model performs poorly on unseen data, it is likely that the model is overfitting. This may happen if the model is too complex and has learned the noise and random fluctuations in the training data, rather than the underlying patterns and relationships that are present.\n",
    "\n",
    "To prevent overfitting, we can use techniques such as regularization, which introduces a penalty term to the objective function to prevent the model from becoming too complex. We can also use cross-validation to evaluate the model on unseen data and tune the hyperparameters of the model to find the optimal balance between model complexity and performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exaplain Lasso Regression\n",
    "\n",
    "Lasso regression is a type of linear regression that uses regularization to reduce the complexity of the model by introducing a penalty term to the objective function. The objective function in lasso regression is defined as:\n",
    "\n",
    "Objective function = Sum of squared residuals + alpha * sum of absolute values of coefficients\n",
    "\n",
    "where the sum of squared residuals is the sum of the squares of the differences between the predicted values and the true values, and the sum of absolute values of coefficients is the sum of the absolute values of the coefficients of the model. The alpha parameter is a hyperparameter that controls the strength of the penalty term and must be set by the user.\n",
    "\n",
    "In lasso regression, the goal is to find the coefficients of the model that minimize the objective function. The penalty term is added to the objective function to prevent overfitting by limiting the complexity of the model. As the value of alpha increases, the penalty term becomes stronger, and the coefficients of the model become smaller. This results in a model with fewer features, which is useful for feature selection.\n",
    "\n",
    "Lasso regression can be used to predict a continuous variable or a binary variable. It is often used in situations where there are a large number of features and the goal is to select a small number of important features for the model. Lasso regression is also useful for handling multicollinearity, which is when there is a strong correlation between the features in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exaplain the Lasso Regression with the Iris Dataset\n",
    "\n",
    "To apply lasso regression to the iris dataset, we first need to prepare the data by splitting it into a training set and a test set. The training set is used to fit the lasso regression model, and the test set is used to evaluate the performance of the model on unseen data.\n",
    "\n",
    "Next, we need to select the features to use in the model. In the iris dataset, there are four features: sepal length, sepal width, petal length, and petal width. We can use all four features, or we can select a subset of the features based on their importance or relevance to the target variable (the species of iris).\n",
    "\n",
    "Once we have selected the features, we can fit the lasso regression model to the training data. To do this, we need to set the value of the alpha hyperparameter, which controls the strength of the penalty term in the objective function. The value of alpha can be determined using cross-validation or by manually searching for the optimal value.\n",
    "\n",
    "Once the lasso regression model is trained, we can use it to predict the species of iris for the samples in the test set. To do this, we provide the model with the features of each sample and use the predicted probability to assign a class label (setosa, versicolor, or virginica) to the sample.\n",
    "\n",
    "Finally, we can evaluate the performance of the lasso regression model on the test set by calculating metrics such as accuracy, precision, and recall. These metrics will give us an idea of how well the model is able to classify the species of iris based on the features. If the performance is satisfactory, we can use the lasso regression model to make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: -0.013130252100840067\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression on Iris Dataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "# Standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit the lasso regression model\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = r2_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# In this example, we first load the iris dataset using scikit-learn's load_iris function. We then split the data into a training set and a test set using the train_test_split function. \n",
    "# Next, we standardize the features using the StandardScaler class to ensure that the features are on the same scale.\n",
    "\n",
    "# We then fit the lasso regression model to the training data using the Lasso class, \n",
    "# setting the value of alpha to 1.0. We use the fit method to fit the model to the data.\n",
    "\n",
    "# Next, we use the model to predict the labels for the samples in the test set using the predict method. \n",
    "# Finally, we evaluate the model's performance on the test set by calculating the accuracy using the accuracy_score function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why you insert the standard Scaler and what are other preprocessing functions ? \n",
    "\n",
    "It is a good practice to standardize the features of a dataset before training a machine learning model, particularly when the features are on different scales. Standardization involves scaling the features so that they have zero mean and unit variance. This is done by subtracting the mean of each feature from each data point and dividing the result by the standard deviation of the feature.\n",
    "\n",
    "Standardization has several benefits. It can help the model converge faster during training, and it can also help the model make more accurate predictions. Standardization can also help prevent the features with larger scales from dominating the objective function, which can lead to better feature selection.\n",
    "\n",
    "There are other preprocessing techniques that can be used to prepare the data for machine learning. For example:\n",
    "\n",
    "* Normalization: Normalization scales the data so that it has a minimum value of 0 and a maximum value of 1. This is done by subtracting the minimum value from each data point and dividing the result by the range (maximum value - minimum value). Normalization is useful when the scale of the features is not important and the model should focus on the rank or order of the data.\n",
    "\n",
    "* Imputation: Imputation is the process of filling in missing values in the data. This can be done by replacing the missing values with the mean, median, or mode of the feature, or by using a more sophisticated method such as multiple imputation.\n",
    "\n",
    "* One-hot encoding: One-hot encoding is a technique used to encode categorical variables as numerical data. It involves creating a new binary feature for each category and assigning a value of 1 to the feature corresponding to the category and 0 to all other features. One-hot encoding is useful for handling categorical variables in machine learning models.\n",
    "\n",
    "There are many other preprocessing techniques that can be used depending on the specific requirements of the dataset and the machine learning model. It is important to choose the appropriate preprocessing techniques for the specific problem at hand to ensure that the model is able to learn the relevant patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Normalize the features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Replace missing values with the mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Encode the target variable as categorical data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what are other preprocessing techiques \n",
    "\n",
    "There are many other preprocessing techniques that can be used to prepare the data for machine learning. Here are a few examples:\n",
    "\n",
    "Encoding ordinal features: Ordinal features are categorical variables that have an inherent order. For example, a feature that represents the size of a shirt as small, medium, or large is an ordinal feature. These features can be encoded as numerical data by assigning a numerical value to each category.\n",
    "\n",
    "* Encoding nominal features: Nominal features are categorical variables that do not have an inherent order. For example, a feature that represents the color of a shirt as red, green, or blue is a nominal feature. These features can be encoded using one-hot encoding or by using a technique called target encoding, which replaces the categorical values with the mean target value for each category.\n",
    "\n",
    "* Removing outliers: Outliers are data points that are significantly different from the majority of the data. These data points can have a negative impact on the performance of the model and should be removed if they are not representative of the underlying patterns and relationships in the data. Outliers can be identified using techniques such as the Z-score or the Tukey method, and they can be removed using the RobustScaler or QuantileTransformer classes in scikit-learn.\n",
    "\n",
    "* Handling imbalanced datasets: Imbalanced datasets are datasets where the class distribution is not balanced, meaning that one class is significantly more prevalent than the other. This can lead to biased models that favor the more prevalent class. To handle imbalanced datasets, we can use techniques such as undersampling, oversampling, and class weighting to balance the class distribution.\n",
    "\n",
    "* Generating synthetic samples: Synthetic samples are artificially generated samples that are used to augment the training data. These samples can be generated using techniques such as SMOTE (Synthetic Minority Over-sampling Technique), which generates synthetic samples of the minority class to balance the class distribution.\n",
    "\n",
    "There are many other preprocessing techniques that can be used depending on the specific requirements of the dataset and the machine learning model. It is important to choose the appropriate preprocessing techniques for the specific problem at hand to ensure that the model is able to learn the relevant patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding ordinal features:\n",
    "\n",
    "# Load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Encode the target variable as ordinal data\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "y_ordinal = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Encoding nominal features:\n",
    "\n",
    "# Load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Encode the target variable using one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Encode the target variable using target encoding\n",
    "from category_encoders import TargetEncoder\n",
    "encoder = TargetEncoder()\n",
    "y_target = encoder.fit_transform(y.reshape(-1, 1), y)\n",
    "\n",
    "# Removing outliers:\n",
    "\n",
    "# Load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Remove outliers using the Z-score method\n",
    "from scipy import stats\n",
    "X_filtered = X[(np.abs(stats.zscore(X)) < 3).all(axis=1)]\n",
    "\n",
    "# Remove outliers using the Tukey method\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_filtered = scaler.fit_transform(X)\n",
    "\n",
    "# Handling imbalanced datasets:\n",
    "\n",
    "# Load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Balance the class distribution using undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersampler = RandomUnderSampler()\n",
    "X_undersampled, y_undersampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "# Balance the class distribution using oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversampler = RandomOverSampler()\n",
    "X_oversampled, y_oversampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "# Balance the class distribution using class weighting\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(class_weight='balanced')\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Generating synthetic samples:\n",
    "\n",
    "# Load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Generate synthetic samples using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_synthetic, y_synthetic = smote.fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to evalute the performance of the lasso regression ? \n",
    "\n",
    "There are several ways to evaluate the performance of a lasso regression model. The most common metric for evaluating the performance of a regression model is the mean squared error (MSE), which measures the average squared difference between the predicted values and the true values. The smaller the MSE, the better the model's performance.\n",
    "\n",
    "To calculate the MSE for a lasso regression model, we can use scikit-learn's mean_squared_error function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = lasso.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('MSE:', mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another metric that is often used to evaluate the performance of a regression model is the root mean squared error (RMSE), which is the square root of the MSE. The RMSE is useful because it is in the same units as the target variable, so it is easier to interpret.\n",
    "\n",
    "To calculate the RMSE, we can simply take the square root of the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the MSE and RMSE, there are other metrics that can be used to evaluate the performance of a lasso regression model. For example:\n",
    "\n",
    "R2 score: The R2 score (also known as the coefficient of determination) is a measure of the proportion of the variance in the target variable that is explained by the model. A value of 1 indicates that the model perfectly explains the variance in the target variable, while a value of 0 indicates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to i evalute the performance of the regularization term ? \n",
    "\n",
    "common approach is to use cross-validation. Cross-validation is a technique that involves dividing the data into a training set and a test set, and training the model on the training set and evaluating it on the test set. This process is repeated multiple times, with different combinations of training and test sets, to get a more robust estimate of the model's performance.\n",
    "\n",
    "To use cross-validation with a lasso regression model, we can use scikit-learn's GridSearchCV function to search for the optimal value of the alpha hyperparameter, which controls the strength of the regularization term.\n",
    "\n",
    "Here is an example of how to use cross-validation to evaluate the performance of the regularization term in a lasso regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Define the range of values to search for alpha\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Create the grid search object\n",
    "lasso = Lasso()\n",
    "grid_search = GridSearchCV(lasso,param_grid, cv=5, return_train_score=True)\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the results of the grid search\n",
    "print(\"Best alpha:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we first define the range of values to search for the alpha hyperparameter using the param_grid dictionary. We then create the GridSearchCV object, setting the number of folds for cross-validation to 5 and specifying that we want to return the training scores.\n",
    "\n",
    "Next, we fit the GridSearchCV object to the data using the fit method. This will train the lasso regression model on the training set and evaluate it on the test set using cross-validation.\n",
    "\n",
    "Finally, we print the results of the grid search, including the best value of alpha and the best score (mean cross-validated score of the best estimator).\n",
    "\n",
    "The mean cross-validated score of the best estimator (also known as the best mean score) is the mean score of the best model among the different folds of the cross-validation. It is a measure of the model's performance on the test set.\n",
    "\n",
    "To get a more complete picture of the model's performance, we can also look at the scores for each fold of the cross-validation. These scores are stored in the cv_results_ attribute of the GridSearchCV object, and they can be accessed using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(scores, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print the mean score and standard deviation for each value of alpha, along with the corresponding value of alpha. This can help us understand how the model's performance changes as we vary the value of alpha.\n",
    "\n",
    "By evaluating the model's performance using cross-validation, we can get a better estimate of how the model will generalize to new data. This is particularly important when we are trying to tune the hyperparameters of the model, such as the alpha value in lasso regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Should i use a confusion matrix here or not ? \n",
    "\n",
    "A confusion matrix is a common tool used to evaluate the performance of a classification model. It is a table that shows the number of true positive, true negative, false positive, and false negative predictions made by the model.\n",
    "\n",
    "In the context of lasso regression, which is a type of regression model rather than a classification model, a confusion matrix would not be an appropriate tool for evaluating the model's performance. Instead, as mentioned earlier, common metrics for evaluating the performance of a regression model include the mean squared error (MSE), root mean squared error (RMSE), and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example of how to implement a lasso regression model on the iris dataset using cross-validation in Python:\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the range of values to search for alpha\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Create the grid search object\n",
    "lasso = Lasso()\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=5, return_train_score=True)\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the results of the grid search\n",
    "print(\"Best alpha:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = grid_search.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Test MSE:\", mse)\n",
    "print(\"Test RMSE:\", rmse\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is lasso regression better than linear regression ? \n",
    "\n",
    "Lasso regression is a type of linear regression that uses a regularization term to reduce the complexity of the model and prevent overfitting. The regularization term is a penalty applied to the coefficients of the model, which shrinks the coefficients towards zero and reduces the number of features that the model uses.\n",
    "\n",
    "One advantage of lasso regression over linear regression is that it can select the most important features of the data and ignore the less important ones. This can be particularly useful in datasets with a large number of features, where linear regression may struggle to identify the most relevant features.\n",
    "\n",
    "Another advantage of lasso regression is that it can help to reduce the variance of the model, which can make it more robust to small changes in the data. This can be particularly useful in cases where the data is noisy or contains outliers.\n",
    "\n",
    "Overall, lasso regression can be a useful tool for linear regression when the goal is to identify the most important features of the data and reduce the complexity of the model. However, it is important to tune the regularization term properly to ensure that the model is not underfitting or overfitting the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can i plot the result of a lasso regression ? \n",
    "\n",
    "To plot the results of a lasso regression model, you can use a scatter plot to visualize the relationship between the target variable and the predicted values. This can be helpful for understanding how well the model is able to capture the underlying trend in the data.\n",
    "\n",
    "Here is an example of how to plot the results of a lasso regression model in Python using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the predicted values against the true values\n",
    "plt.scatter(y_test, y_pred)\n",
    "\n",
    "# Add a diagonal line to indicate where the points would fall if the model was perfect\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "\n",
    "# Add axis labels and a title\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Lasso Regression Results')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# are there any other ways to plot ? \n",
    "\n",
    "Yes, there are several other ways to visualize the performance of a lasso regression model. Here are a few examples:\n",
    "\n",
    "* Residual plot: As mentioned earlier, a residual plot is a scatter plot of the residuals (difference between the true values and the predicted values) as a function of the true values. A residual plot can help you understand how well the model is fitting the data and identify any patterns in the residuals that may indicate problems with the model.\n",
    "\n",
    "* Predicted vs. actual plot: This is similar to the scatter plot described earlier, but it plots the predicted values on the x-axis and the true values on the y-axis. This can be helpful for visualizing the overall accuracy of the model.\n",
    "\n",
    "* Error distribution plot: This plot shows the distribution of the errors (difference between the true values and the predicted values) for the entire dataset. If the errors are evenly distributed around zero, it indicates that the model is performing well.\n",
    "\n",
    "* Learning curve: A learning curve plots the model's performance (e.g. MSE or RMSE) as a function of the number of training examples. This can be helpful for understanding how the model's performance changes as the number of training examples increases.\n",
    "\n",
    "* Feature importance plot: If you are using a lasso regression model to identify the most important features of the data, you can create a plot to visualize the feature importances. This can be helpful for understanding which features are driving the model's predictions.\n",
    "\n",
    "These are just a few examples of the types of plots that you can use to visualize the performance of a lasso regression model. The appropriate plot will depend on the specific goals of your analysis and the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot the residuals\n",
    "plt.scatter(y_test, residuals)\n",
    "\n",
    "# Add a horizontal line at y=0 to indicate the expected residuals\n",
    "plt.hlines(0, y_test.min(), y_test.max(), 'k', lw=2)\n",
    "\n",
    "# Add axis labels and a title\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Predicted vs. actual plot:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the predicted values against the true values\n",
    "plt.scatter(y_pred, y_test)\n",
    "\n",
    "# Add a diagonal line to indicate where the points would fall if the model was perfect\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "\n",
    "# Add axis labels and a title\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('True Values')\n",
    "plt.title('Predicted vs. Actual Values')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Error distribution plot:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the errors\n",
    "errors = y_test - y_pred\n",
    "\n",
    "# Plot the errors\n",
    "plt.hist(errors, bins=20)\n",
    "\n",
    "# Add a vertical line at y=0 to indicate the expected errors\n",
    "plt.vlines(0, 0, errors.size, 'k', lw=2)\n",
    "\n",
    "# Add axis labels and a title\n",
    "plt.xlabel('Errors')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Error Distribution')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Learning curve:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Create the learning curve plot\n",
    "train_sizes, train_scores, test_scores = learning_cur\n",
    "\n",
    "# Calculate the mean and standard deviation of the training and test scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(train_sizes, train_mean, 'r', label='Training score')\n",
    "plt.plot(train_sizes, test_mean, 'g', label='Cross-validation score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='g')\n",
    "\n",
    "# Add axis labels and a title\n",
    "plt.xlabel('Number of Training Examples')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# This code uses scikit-learn's learning_curve function to generate the learning curve plot. \n",
    "# It calculates the mean and standard deviation of the training and test scores for different sizes\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6efacc00bcce593500b9538249af7153ff1bd5a980aeb167913446d147143c22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
