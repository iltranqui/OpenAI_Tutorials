{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris Dataset is a well-known dataset that contains measurements of flowers from three species of iris. It includes four features (sepal length, sepal width, petal length, and petal width) and a target (species of iris). Here is an example of how you could perform a linear regression on the Iris Dataset using the scikit-learn library:\n",
    "\n",
    "This code first loads the Iris Dataset into a pandas DataFrame, and then selects the features (X) and target (y). It then splits the data into a training set and a test set using the train_test_split() function from scikit-learn.\n",
    "\n",
    "Next, it creates a linear regression model using the LinearRegression class, and fits the model to the training data using the fit() method. Finally, it makes predictions on the test data using the predict() method, and prints the predictions.\n",
    "\n",
    "Note that in this example, the target (y) is the species of iris, which is a categorical variable. Linear regression is generally used for predicting a continuous target variable, so it may not be the most appropriate method for this dataset. However, this example demonstrates the basic steps for performing a linear regression using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris Dataset into a pandas DataFrame\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "                 names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "# Select the features and target\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y = df['species']\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the predictions\n",
    "print(y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do i need to split into tain and test set ? \n",
    "\n",
    "It is important to split your data into a training set and a test set because it allows you to evaluate the performance of your model on unseen data.\n",
    "\n",
    "The training set is used to fit the model, i.e., to train the model using a set of input features and corresponding target values. The test set is used to evaluate the model's performance, by applying the trained model to a different set of input features and comparing the predicted target values to the actual target values.\n",
    "\n",
    "By evaluating the model's performance on the test set, you can get a better idea of how well the model will generalize to new, unseen data. This is important because you want your model to be able to accurately predict the target values for new data, not just the data it was trained on.\n",
    "\n",
    "Splitting the data into a training set and a test set also allows you to tune the model's hyperparameters (e.g., the learning rate in a neural network) using the training set, and then evaluate the model's performance on the test set to see if the model is overfitting or underfitting.\n",
    "\n",
    "Overall, splitting the data into a training set and a test set is a crucial step in the model development process, as it helps you to evaluate the performance of your model and ensure that it will generalize well to new, unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do i need to always split between training, testing and validation ? \n",
    "\n",
    "It is generally a good idea to split your data into a training set, a validation set, and a test set when building machine learning models.\n",
    "\n",
    "The training set is used to fit the model, i.e., to train the model using a set of input features and corresponding target values. The validation set is used to tune the model's hyperparameters, i.e., to select the best set of hyperparameters for the model based on the model's performance on the validation set. The test set is used to evaluate the model's final performance, by applying the trained and tuned model to a different set of input features and comparing the predicted target values to the actual target values.\n",
    "\n",
    "Splitting the data into a training set, a validation set, and a test set allows you to evaluate the model's performance on unseen data at multiple stages in the model development process. It helps you to ensure that the model is not overfitting to the training data, and that it will generalize well to new, unseen data.\n",
    "\n",
    "In some cases, you may not need to use a validation set, especially if you have a large dataset and don't need to tune many hyperparameters. In this case, you can split the data into a training set and a test set, and use the test set to evaluate the model's performance.\n",
    "\n",
    "Overall, it is important to evaluate the model's performance on unseen data, and splitting the data into a training set, a validation set, and a test set is a useful way to do this. It helps you to ensure that the model is performing well and will generalize well to new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how can you evalute the performance of the Linear Regression of before ? \n",
    "\n",
    "To evaluate the performance of the linear regression model in the previous example, you can use a variety of evaluation metrics. Some common evaluation metrics for regression problems include:\n",
    "\n",
    "Mean Squared Error (MSE): This measures the average squared difference between the predicted values and the actual values. A lower value indicates a better fit.\n",
    "\n",
    "Mean Absolute Error (MAE): This measures the average absolute difference between the predicted values and the actual values. A lower value indicates a better fit.\n",
    "\n",
    "R-squared: This is a measure of how well the model fits the data. It ranges from 0 to 1, with a higher value indicating a better fit.\n",
    "\n",
    "To calculate these evaluation metrics, you can use the mean_squared_error(), mean_absolute_error(), and r2_score() functions from the scikit-learn library. Here is an example of how you could use these functions to evaluate the performance of the linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'MSE: {mse:.2f}')\n",
    "print(f'MAE: {mae:.2f}')\n",
    "print(f'R2: {r2:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which among the three evaluation methods is the best ? \n",
    "\n",
    "It is not always possible to determine which evaluation metric is the best to use, as it depends on the specific characteristics of your dataset and the goals of your modeling task. However, here are some general guidelines for selecting evaluation metrics:\n",
    "\n",
    "Mean Squared Error (MSE) is a common choice for regression problems. It is sensitive to outliers, so it may not be the best choice if your dataset has a lot of outliers.\n",
    "\n",
    "Mean Absolute Error (MAE) is also a common choice for regression problems. It is less sensitive to outliers than MSE, so it may be a better choice if your dataset has a lot of outliers.\n",
    "\n",
    "R-squared is a measure of how well the model fits the data. It ranges from 0 to 1, with a higher value indicating a better fit. However, R-squared is not always the best choice as a single metric, because it can be influenced by the number of features in the model and the complexity of the model.\n",
    "\n",
    "Overall, it is important to consider the characteristics of your dataset and the goals of your modeling task when selecting an evaluation metric. You may want to use multiple evaluation metrics to get a more comprehensive view of the model's performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# which is best for large dataset and small dataasets ? \n",
    "\n",
    "The choice of evaluation metric may depend on the size of your dataset and the specific characteristics of your data. Here are some general guidelines for selecting evaluation metrics for large and small datasets:\n",
    "\n",
    "For large datasets, mean squared error (MSE) and mean absolute error (MAE) are often good choices for regression problems. These metrics are sensitive to the average deviation of the predictions from the actual values, and are not sensitive to the scale of the data. They may be less sensitive to the presence of outliers in large datasets, as the average is taken over a large number of samples.\n",
    "\n",
    "For small datasets, mean squared error (MSE) and mean absolute error (MAE) may be less reliable, as the average deviation may be influenced by a small number of outliers or anomalies. In this case, you may want to consider using other evaluation metrics that are less sensitive to the presence of outliers, such as median absolute error or the coefficient of determination (R-squared).\n",
    "\n",
    "Overall, it is important to consider the characteristics of your dataset and the goals of your modeling task when selecting an evaluation metric. You may want to use multiple evaluation metrics to get a more comprehensive view of the model's performance, and to account for the specific characteristics of your data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the parameters of the Linear Regression function ? \n",
    "\n",
    "The LinearRegression function from the scikit-learn library has the following parameters:\n",
    "\n",
    "fit_intercept (bool, optional, default=True): Whether to calculate the intercept for the model. If set to False, no intercept will be used in the model.\n",
    "\n",
    "normalize (bool, optional, default=False): Whether to normalize the input features before fitting the model. If set to True, the input features will be normalized by subtracting the mean and dividing by the l2-norm.\n",
    "\n",
    "copy_X (bool, optional, default=True): Whether to copy the input features before fitting the model. If set to False, the input features will not be copied, and any changes to the input features after fitting the model will be reflected in the model.\n",
    "\n",
    "n_jobs (int or None, optional, default=None): The number of jobs to use for the computation. If set to None, all available CPU cores will be used.\n",
    "\n",
    "Here is an example of how you could use these parameters to create a linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a linear regression model with the fit_intercept parameter set to False\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Create a linear regression model with the normalize parameter set to True\n",
    "model = LinearRegression(normalize=True)\n",
    "\n",
    "# Create a linear regression model with the n_jobs parameter set to 2\n",
    "model = LinearRegression(n_jobs=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust these parameters based on your specific requirements and the characteristics of your dataset. For example, you may want to set fit_intercept to False if you know that the data does not have an intercept, or set normalize to True if the input features have different scales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is the difference between the condifence interval and the prediction intreval ? \n",
    "\n",
    "The confidence interval and the prediction interval are two types of intervals that are used to estimate the uncertainty of a statistical estimate.\n",
    "\n",
    "A confidence interval is an interval estimate of a population parameter. It is a range of values that is expected to contain the true value of the parameter with a certain level of confidence. For example, a 95% confidence interval for the mean of a population indicates that the true mean of the population is expected to be within the interval with a probability of 95%.\n",
    "\n",
    "A prediction interval is an interval estimate of a future observation. It is a range of values that is expected to contain a future observation with a certain level of confidence. For example, a 95% prediction interval for a future observation indicates that the future observation is expected to be within the interval with a probability of 95%.\n",
    "\n",
    "The main difference between the confidence interval and the prediction interval is that the confidence interval is used to estimate a population parameter, while the prediction interval is used to estimate a future observation. Confidence intervals are often used to assess the precision of statistical estimates, while prediction intervals are used to assess the uncertainty of future observations.\n",
    "\n",
    "In general, prediction intervals are wider than confidence intervals, as they include the uncertainty associated with predicting a future observation, while confidence intervals only include the uncertainty associated with estimating a population parameter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# can you generate preidction intervals and confidence intervals for linear regression of ebfore ? \n",
    "\n",
    "To generate prediction intervals and confidence intervals for a linear regression model in scikit-learn, you can use the predict() function with the return_std parameter set to True. This will return the predicted values as well as the standard deviation of the predictions, which can be used to calculate the prediction intervals and confidence intervals.\n",
    "\n",
    "Here is an example of how you could generate prediction intervals and confidence intervals for a linear regression model using the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris Dataset into a pandas DataFrame\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "                 names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "# Select the features and target\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y = df['species']\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data, with the standard deviation of the predictions\n",
    "y_pred, y_pred_std = model.predict(X_test, return_std=True)\n",
    "\n",
    "# Calculate the 95% prediction interval\n",
    "prediction_interval = y_pred_std * 1.96\n",
    "\n",
    "# Calculate the 95% confidence interval for the model parameters\n",
    "num_samples = len(X_train)\n",
    "degrees_of_freedom = num_samples - model.rank_\n",
    "t = stats.t.ppf(0.975, degrees_of_freedom)\n",
    "sigma = np.sqrt(1 / (num_samples - model.rank_) * np.sum((y_train - model.predict(X_train)) ** 2))\n",
    "param_std = sigma * t\n",
    "confidence_interval = param_std\n",
    "\n",
    "# Print the prediction intervals and confidence intervals\n",
    "print(f'Prediction interval: {prediction_interval:.2f}')\n",
    "print(f'Confidence interval: {confidence_interval:.2f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why do you calculate the degrees of freedom ? \n",
    "\n",
    "The degrees of freedom is a measure of the number of independent observations in a statistical analysis. It is used to calculate the standard error of the estimate, which is a measure of the precision of the estimate.\n",
    "\n",
    "In the context of linear regression, the degrees of freedom is the number of observations in the training set minus the number of model parameters. The model parameters include the intercept and the coefficients for each feature.\n",
    "\n",
    "The degrees of freedom is used to calculate the confidence interval for the model parameters. The confidence interval is an interval estimate of the true values of the model parameters, and is calculated using the standard error of the estimate and a t-value from the t-distribution.\n",
    "\n",
    "In the example code above, the degrees of freedom is calculated using the rank_ attribute of the LinearRegression model, which returns the number of model parameters. The degrees of freedom is then used to calculate the standard error of the estimate and the confidence interval for the model parameters.\n",
    "\n",
    "Overall, the degrees of freedom is an important concept in statistical analysis, as it is used to calculate the precision of statistical estimates and to assess the uncertainty of the estimates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what does the number of degrees of freedom change for my model ? \n",
    "\n",
    "The number of degrees of freedom can affect the precision and accuracy of your model in several ways. Here are some ways in which the number of degrees of freedom can impact your model:\n",
    "\n",
    "The number of degrees of freedom determines the t-value used to calculate the confidence interval for the model parameters. The t-value is a critical value from the t-distribution, and is used to determine the width of the confidence interval. A larger number of degrees of freedom corresponds to a smaller t-value, which results in a narrower confidence interval. A narrower confidence interval means that the estimates of the model parameters are more precise, as they are based on a larger number of independent observations.\n",
    "\n",
    "The number of degrees of freedom can also affect the performance of the model on unseen data. If the model has a large number of degrees of freedom, it may be overfitted to the training data, and may not generalize well to new, unseen data. On the other hand, if the model has a small number of degrees of freedom, it may be underfitted, and may not capture the underlying patterns in the data.\n",
    "\n",
    "Overall, the number of degrees of freedom is an important factor to consider when building and evaluating a machine learning model. It can affect the precision of the model's estimates and the model's ability to generalize to new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is stats.t.ppf ? \n",
    "\n",
    "stats.t.ppf() is a function from the scipy.stats library that returns the inverse of the cumulative distribution function of the t-distribution. It is often used to calculate critical values from the t-distribution, which are used to construct confidence intervals or to conduct statistical hypothesis tests.\n",
    "\n",
    "The ppf stands for \"percent point function,\" which is another term for the inverse cumulative distribution function.\n",
    "\n",
    "The stats.t.ppf() function takes two arguments: the probability point and the degrees of freedom. The probability point is the probability that a random variable from the t-distribution is less than or equal to the critical value. The degrees of freedom is a measure of the number of independent observations in the statistical analysis.\n",
    "\n",
    "Here is an example of how you could use the stats.t.ppf() function to calculate a critical value from the t-distribution:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Calculate the 95% critical value from the t-distribution with 10 degrees of freedom\n",
    "critical_value = t.ppf(0.95, 10)\n",
    "print(critical_value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the critical value from the t-distribution with a probability of 95% and 10 degrees of freedom. The resulting critical value can be used to construct a 95% confidence interval or to conduct a statistical hypothesis test.\n",
    "\n",
    "Overall, the stats.t.ppf() function is a useful tool for working with the t-distribution in statistical analysis. It allows you to calculate critical values from the t-distribution, which can be used to assess the uncertainty of statistical estimates and to make statistical inferences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can i apply r2 score, MSE and MAE on the prediction and confidence interval ? \n",
    "\n",
    "The R-squared, mean squared error (MSE), and mean absolute error (MAE) are evaluation metrics that are commonly used to assess the performance of a machine learning model. These metrics can be applied to the predictions and confidence intervals of a linear regression model to evaluate the model's performance.\n",
    "\n",
    "To apply the R-squared metric to the predictions and confidence intervals of a linear regression model, you can use the r2_score() function from the scikit-learn library. This function takes the true values of the target variable and the predicted values as arguments, and returns the R-squared score.\n",
    "\n",
    "Here is an example of how you could apply the R-squared metric to the predictions of a linear regression model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate the R-squared score for the predictions\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the MSE and MAE metrics to the predictions and confidence intervals of a linear regression model, you can use the mean_squared_error() and mean_absolute_error() functions from the scikit-learn library, respectively. These functions take the true values of the target variable and the predicted values as arguments, and return the MSE and MAE scores.\n",
    "\n",
    "Here is an example of how you could apply the MSE and MAE metrics to the predictions of a linear regression model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Calculate the MSE and MAE for the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MSE: {mse:.2f}')\n",
    "print(f'MAE: {mae:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that these evaluation metrics are typically applied to the predictions of the model, rather than the confidence intervals. The confidence intervals are used to assess the uncertainty of the predictions, while the evaluation metrics are used to assess the accuracy of the predictions. However, you can use the evaluation metrics to compare the performance of different models with different levels of uncertainty, or to compare the performance of the same model with different levels of confidence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# after determining a predicition and confidence interval, is there anything morw i need to do ? \n",
    "\n",
    "After determining the prediction intervals and confidence intervals for a linear regression model, there are several steps you may want to consider to further assess the performance of the model and to improve its accuracy:\n",
    "\n",
    "Validate the model using additional data: You may want to use additional data, such as a validation set or cross-validation, to confirm that the model is generalizable and to identify any potential overfitting. This can help you to ensure that the model will perform well on unseen data.\n",
    "\n",
    "Evaluate the model using multiple evaluation metrics: You may want to use multiple evaluation metrics, such as the R-squared score, mean squared error (MSE), and mean absolute error (MAE), to get a more comprehensive view of the model's performance. This can help you to identify any potential weaknesses in the model and to identify areas for improvement.\n",
    "\n",
    "Fine-tune the model's hyperparameters: You may want to try adjusting the model's hyperparameters, such as the learning rate or regularization strength, to improve the model's performance. You can use techniques like grid search or random search to find the optimal hyperparameters for the model.\n",
    "\n",
    "Consider using different modeling techniques: You may want to try using different modeling techniques, such as decision trees or support vector machines, to see if they perform better on the data. You can compare the performance of different models using the evaluation metrics and choose the model that performs the best.\n",
    "\n",
    "Overall, there are many steps you can take to further assess and improve the performance of a linear regression model after determining the prediction intervals and confidence intervals. These steps can help you to build a more accurate and robust model that is better suited to your specific needs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# are there any hyperparameters in a linear regression ? \n",
    "\n",
    "In general, a linear regression model does not have any hyperparameters. Linear regression is a simple modeling technique that is based on the assumption that the target variable is linearly related to the input features. As such, it does not have any adjustable parameters that can be tuned to improve the model's performance.\n",
    "\n",
    "However, some variations of linear regression, such as ridge regression and lasso regression, do have hyperparameters that can be adjusted to improve the model's performance. These variations of linear regression are known as regularized linear regression models, and they introduce additional constraints on the model parameters to reduce the complexity of the model and prevent overfitting.\n",
    "\n",
    "For example, ridge regression introduces a L2 regularization term to the model, which penalizes large coefficients and helps to reduce the complexity of the model. The strength of the regularization term is controlled by a hyperparameter called the regularization strength, which can be adjusted to find the optimal balance between model complexity and model accuracy.\n",
    "\n",
    "Similarly, lasso regression introduces a L1 regularization term to the model, which has the effect of setting some coefficients to zero and effectively selecting a subset of features to use in the model. The strength of the regularization term is controlled by a hyperparameter called the regularization strength, which can be adjusted to find the optimal balance between model complexity and model accuracy.\n",
    "\n",
    "Overall, while linear regression does not have any hyperparameters, regularized linear regression models do have hyperparameters that can be adjusted to improve the model's performance. Tuning these hyperparameters can help you to build a more accurate and robust linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code generates 20 random values for data1 and data2 and performs linear regression to fit a line to the data. \n",
    "# The mean, variance, and covariance functions are used to calculate statistical measures of the data, \n",
    "# and the linear_regression function uses these measures to fit the line.\n",
    "# The resulting slope and intercept of the line are printed to the console.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the mean of the given data\n",
    "def mean(data):\n",
    "  return sum(data) / len(data)\n",
    "\n",
    "# Calculate the variance of the given data\n",
    "def variance(data):\n",
    "  mu = mean(data)\n",
    "  return sum((x - mu) ** 2 for x in data) / len(data)\n",
    "\n",
    "# Calculate the covariance of the given data\n",
    "def covariance(data1, data2):\n",
    "  mu1 = mean(data1)\n",
    "  mu2 = mean(data2)\n",
    "  return sum((x1 - mu1) * (x2 - mu2) for x1, x2 in zip(data1, data2)) / len(data1)\n",
    "\n",
    "# Perform linear regression on the given data\n",
    "def linear_regression(data1, data2):\n",
    "  # Calculate the slope of the regression line\n",
    "  slope = covariance(data1, data2) / variance(data1)\n",
    "  # Calculate the intercept of the regression line\n",
    "  intercept = mean(data2) - slope * mean(data1)\n",
    "  return (slope, intercept)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Generate some random data\n",
    "data1 = np.random.normal(10, 5, 20)\n",
    "data2 = np.random.normal(20, 5, 20)\n",
    "\n",
    "# Perform linear regression\n",
    "slope, intercept = linear_regression(data1, data2)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Slope: {slope:.2f}\")\n",
    "print(f\"Intercept: {intercept:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
